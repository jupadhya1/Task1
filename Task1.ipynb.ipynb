{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[126]:\n",
    "\n",
    "# Importing all necessary Modules\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "import itertools\n",
    "from scipy.stats import describe\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "sns.set(style = 'darkgrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Part 1: Modeling Challenge\n",
    "\n",
    "# * Step 1: Read in the dataframe\n",
    "# * Step 2: Add the columns to the dataframe\n",
    "\n",
    "# In[74]:\n",
    "\n",
    "\n",
    "data_columns = pd.read_csv('./data/field_names.txt', sep='\\n', header=None)\n",
    "\n",
    "col_val = []\n",
    "for i in range(len(data_col_val)):\n",
    "    col = data_col_val.iloc[[i]].values[0][0]\n",
    "    col_val.append(col)\n",
    "\n",
    "\n",
    "# In[75]:\n",
    "\n",
    "\n",
    "df = pd.read_csv('./data/breast-cancer.csv')\n",
    "df.col_val = col_val\n",
    "\n",
    "\n",
    "# In[76]:\n",
    "\n",
    "\n",
    "df.head(5)\n",
    "\n",
    "\n",
    "# In[77]:\n",
    "\n",
    "\n",
    "df.info()\n",
    "\n",
    "\n",
    "# In[78]:\n",
    "\n",
    "\n",
    "feature_mean_value= list(df.col_val[1:11])\n",
    "feature_standard_err= list(df.col_val[11:20])\n",
    "features_worst=list(df.col_val[21:31])\n",
    "print(feature_mean_value)\n",
    "print(\"....................................\")\n",
    "print(feature_standard_err)\n",
    "print(\"....................................\")\n",
    "print(features_worst)\n",
    "\n",
    "\n",
    "# In[79]:\n",
    "\n",
    "\n",
    "df['diagnosis']=df['diagnosis'].map({'M':1,'B':0})\n",
    "df.describe()\n",
    "\n",
    "\n",
    "# In[80]:\n",
    "\n",
    "\n",
    "sns.countplot(df['diagnosis'],label=\"Count\")\n",
    "\n",
    "\n",
    "# In[81]:\n",
    "\n",
    "\n",
    "correlation=df[feature_mean_value].correlation()\n",
    "plt.figure(figsize=(14,14))\n",
    "sns.heatmap(correlation,cbar=True,square=True,annot=True,fmt='.2f',annot_kws={'size': 15},xticklabels=feature_mean_value,yticklabels=feature_mean_value,cmap='coolwarm')\n",
    "\n",
    "\n",
    "# In[82]:\n",
    "\n",
    "\n",
    "pred_var=['texture_mean','perimeter_mean','smoothness_mean','compactness_mean','symmetry_mean']\n",
    "train,test=train_test_split(df,test_size=0.3)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "\n",
    "# In[83]:\n",
    "\n",
    "\n",
    "train_X=train[pred_var]\n",
    "train_y=train.diagnosis\n",
    "test_X=test[pred_var]\n",
    "test_y=test.diagnosis\n",
    "\n",
    "\n",
    "# In[84]:\n",
    "\n",
    "\n",
    "model=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "\n",
    "# In[85]:\n",
    "\n",
    "\n",
    "model.fit(train_X,train_y)\n",
    "\n",
    "\n",
    "# In[86]:\n",
    "\n",
    "\n",
    "prediction=model.predict(test_X)\n",
    "\n",
    "\n",
    "# In[87]:\n",
    "\n",
    "\n",
    "prediction=model.predict(test_X)\n",
    "\n",
    "\n",
    "# In[88]:\n",
    "\n",
    "\n",
    "metrics.accuracy_score(prediction,test_y)\n",
    "\n",
    "\n",
    "# In[89]:\n",
    "\n",
    "\n",
    "## Now lets try the same problem with Neural Network Approach \n",
    "\n",
    "\n",
    "# In[90]:\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "# In[91]:\n",
    "\n",
    "\n",
    "df.shape\n",
    "\n",
    "\n",
    "# In[92]:\n",
    "\n",
    "\n",
    "\"\"\"\"\n",
    "The data has 568 rows, each representing a single patient, with 28 measurements each (note: there are 32 col_val, but column 1 is an ID, column 2 is a diagnosis, and the 32nd is not a measurement)\n",
    "\n",
    "We are now going to proceed by massaging the data. The following steps will be taken to prepare the data for our neural network:\n",
    "\n",
    "Replace the diagnosis of \"M\" for malignant with 1, and \"B\" for benign with 0\n",
    "Remove the 32nd column (not sure why this is included, it is a column of 'NaN' values. Perhaps this was erroneously included in the dataset)\n",
    "Remove the 'id' column. These values will not be needed to create our neural network.\n",
    "Split our dataset into two: a calssification vector (diagnosis) and our feature matrix (the 28 measurements)\n",
    "Normalize our feature matrix by forcing the mean of each measurement to 0, and dividing each measurement by the maximum value of that measurement in the dataset.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# In[96]:\n",
    "\n",
    "\n",
    "data=df\n",
    "\n",
    "\n",
    "# In[98]:\n",
    "\n",
    "\n",
    "X = data.iloc[:, 2:].values\n",
    "y = data.iloc[:, 1].values\n",
    "\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "y = labelencoder_X_1.fit_transform(y)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
    "\n",
    "#Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "# In[99]:\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "# In[100]:\n",
    "\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "\n",
    "# In[101]:\n",
    "\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim=16, init='uniform', activation='relu', input_dim=30))\n",
    "# Adding dropout to prevent overfitting\n",
    "classifier.add(Dropout(p=0.1))\n",
    "\n",
    "\n",
    "# In[102]:\n",
    "\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim=16, init='uniform', activation='relu'))\n",
    "# Adding dropout to prevent overfitting\n",
    "classifier.add(Dropout(p=0.1))\n",
    "\n",
    "\n",
    "# In[103]:\n",
    "\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))\n",
    "\n",
    "\n",
    "# In[104]:\n",
    "\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# In[105]:\n",
    "\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size=100, nb_epoch=150)\n",
    "# Long scroll ahead but worth\n",
    "# The batch size and number of epochs have been set using trial and error. Still looking for more efficient ways. Open to suggestions. \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "Batch size defines number of samples that going to be propagated through the network.\n",
    "\n",
    "An Epoch is a complete pass through all the training data.\n",
    "\n",
    "\n",
    "# In[106]:\n",
    "\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "\n",
    "# In[107]:\n",
    "\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "# In[108]:\n",
    "\n",
    "\n",
    "print(\"Our accuracy is {}%\".format(((cm[0][0] + cm[1][1])/57)*100))\n",
    "\n",
    "\n",
    "# In[109]:\n",
    "\n",
    "\n",
    "sns.heatmap(cm,annot=True)\n",
    "plt.savefig('h.png')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Try with Support Vector Machine\n",
    "\n",
    "\n",
    "# In[110]:\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# In[111]:\n",
    "\n",
    "\n",
    "bc=df\n",
    "\n",
    "\n",
    "# In[115]:\n",
    "\n",
    "\n",
    "bcs = pd.DataFrame(preprocessing.scale(bc.ix[:,2:32]))\n",
    "bcs.col_val = list(bc.ix[:,2:32].col_val)\n",
    "bcs['diagnosis'] = bc['diagnosis']\n",
    "\n",
    "\n",
    "# In[116]:\n",
    "\n",
    "\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "p = sns.PairGrid(bcs.ix[:,20:32], hue = 'diagnosis', palette = 'Reds')\n",
    "p.map_upper(plt.scatter, s = 20, edgecolor = 'w')\n",
    "p.map_diag(plt.hist)\n",
    "p.map_lower(sns.kdeplot, cmap = 'GnBu_d')\n",
    "p.add_legend()\n",
    "\n",
    "p.figsize = (30,30)\n",
    "\n",
    "\n",
    "# In[117]:\n",
    "\n",
    "\n",
    "mbc = pd.melt(bcs, \"diagnosis\", var_name=\"measurement\")\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "p = sns.violinplot(ax = ax, x=\"measurement\", y=\"value\", hue=\"diagnosis\", split = True, data=mbc, inner = 'quartile', palette = 'Set2');\n",
    "p.set_xticklabels(rotation = 90, labels = list(bcs.col_val));\n",
    "\n",
    "\n",
    "# In[120]:\n",
    "\n",
    "\n",
    "X = bcs.ix[:,0:30]\n",
    "\n",
    "y = bcs['diagnosis']\n",
    "class_names = list(y.unique())\n",
    "\n",
    "\n",
    "# In[121]:\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# In[124]:\n",
    "\n",
    "\n",
    "# Model score \n",
    "svc = SVC(kernel = 'linear',C=.1, gamma=10, probability = True)\n",
    "svc.fit(X,y)\n",
    "y_pred = svc.fit(X_train, y_train).predict(X_test)\n",
    "t = pd.DataFrame(svc.predict_proba(X_test))\n",
    "svc.score(X_train,y_train), svc.score(X_test, y_test)\n",
    "\n",
    "\n",
    "# In[127]:\n",
    "\n",
    "\n",
    "mtrx = confusion_matrix(y_test,y_pred)\n",
    "np.set_printoptions(precision = 2)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(mtrx,classes=class_names,title='Confusion matrix, without normalization')\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(mtrx, classes=class_names, normalize = True, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
